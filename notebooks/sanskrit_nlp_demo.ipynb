{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanskrit NLP Components for Vedic Language Model\n",
    "\n",
    "This notebook demonstrates the core Sanskrit NLP components we've implemented for the Vedic Language Model:\n",
    "1. Sandhi processing (phonological junctions)\n",
    "2. Pāṇinian grammar validation\n",
    "3. Basic Sanskrit tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "\n",
    "from src.vlm.grammar.sandhi import SandhiProcessor\n",
    "from src.vlm.grammar.ashtadhyayi import AshtadhyayiEngine\n",
    "from src.vlm.core.tokenizer import SanskritTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sandhi Processing\n",
    "\n",
    "Demonstrate Sanskrit sandhi (phonological junction) rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the sandhi processor\n",
    "sandhi_processor = SandhiProcessor()\n",
    "\n",
    "# Test sandhi application\n",
    "print(\"Sandhi Application Demo:\")\n",
    "test_cases = [\n",
    "    (\"rama\", \"iva\", \"rameva\"),  # a + i = e\n",
    "    (\"deva\", \"atra\", \"devātra\"),  # a + a = ā\n",
    "    (\"gaccha\", \"uta\", \"gacchota\"),  # a + u = o\n",
    "]\n",
    "\n",
    "for first, second, expected in test_cases:\n",
    "    result = sandhi_processor.apply(first, second)\n",
    "    print(f\"Sandhi rule: {first} + {second} = {result}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Correct: {result == expected}\")\n",
    "    print()\n",
    "\n",
    "# Test sandhi reversal\n",
    "print(\"Sandhi Reversal Demo:\")\n",
    "test_cases = [\n",
    "    \"devātra\",  # deva + atra\n",
    "    \"rameva\",   # rama + iva\n",
    "    \"gacchota\"  # gaccha + uta\n",
    "]\n",
    "\n",
    "for combined in test_cases:\n",
    "    result = sandhi_processor.reverse(combined)\n",
    "    print(f\"Sandhi reversal of '{combined}' = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grammar Validation\n",
    "\n",
    "Demonstrate the Pāṇinian grammar validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the grammar engine\n",
    "grammar_engine = AshtadhyayiEngine()\n",
    "\n",
    "# Sample texts for grammar validation\n",
    "print(\"Grammar Validation Demo:\")\n",
    "\n",
    "# Valid sentences\n",
    "valid_sentences = [\n",
    "    \"rāmaḥ vanam gacchati\",  # Rama goes to the forest\n",
    "    \"devāḥ yajñam rakṣanti\"   # The gods protect the sacrifice\n",
    "]\n",
    "\n",
    "for sentence in valid_sentences:\n",
    "    is_valid = grammar_engine.validate(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Valid: {is_valid}\")\n",
    "    print()\n",
    "\n",
    "# Invalid sentences\n",
    "invalid_sentences = [\n",
    "    \"rāma vanam gacchati\",    # Missing visarga on subject\n",
    "    \"gacchati rāmaḥ vanam\"    # Wrong word order (VSO instead of SOV)\n",
    "]\n",
    "\n",
    "for sentence in invalid_sentences:\n",
    "    is_valid = grammar_engine.validate(sentence)\n",
    "    corrected = grammar_engine.correct(sentence)\n",
    "    print(f\"Invalid Sentence: {sentence}\")\n",
    "    print(f\"Valid: {is_valid}\")\n",
    "    print(f\"Corrected: {corrected}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grammatical Parsing\n",
    "\n",
    "Parse a Sanskrit sentence using the Aṣṭādhyāyī engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Parse a valid sentence\n",
    "print(\"Grammatical Parsing Demo:\")\n",
    "sentence = \"rāmaḥ vanam gacchati\"  # Rama goes to the forest\n",
    "analysis = grammar_engine.parse_sentence(sentence)\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(\"Analysis:\")\n",
    "for i, word_analysis in enumerate(analysis[\"words\"]):\n",
    "    print(f\"  Word {i+1}: {word_analysis['text']}\")\n",
    "    for key, value in word_analysis.items():\n",
    "        if key != \"text\":\n",
    "            print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanskrit Tokenization\n",
    "\n",
    "Demonstrate the Sanskrit tokenizer (for transliterated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = SanskritTokenizer()\n",
    "    \n",
    "    # Test tokenization\n",
    "    text = \"rāmāyaṇam\"  # Sanskrit word for 'Ramayana'\n",
    "    tokens = tokenizer._tokenize(text)\n",
    "    token_ids = [tokenizer._convert_token_to_id(token) for token in tokens]\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    \n",
    "    # Reconstruct text from tokens\n",
    "    reconstructed = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"Reconstructed: {reconstructed}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing tokenizer: {e}\")\n",
    "    print(\"Using simple character tokenization as fallback:\")\n",
    "    text = \"rāmāyaṇam\"  # Sanskrit word for 'Ramayana'\n",
    "    tokens = list(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Simple tokenization: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps for Sanskrit NLP Components\n",
    "\n",
    "Future improvements for these components:\n",
    "\n",
    "1. Sandhi Processor:\n",
    "   - Implement comprehensive sandhi rule set\n",
    "   - Add probabilistic sandhi splitting for ambiguous cases\n",
    "   - Support Devanagari text directly\n",
    "\n",
    "2. Aṣṭādhyāyī Grammar Engine:\n",
    "   - Implement full rule set from Pāṇini's original work\n",
    "   - Add support for all declension and conjugation patterns\n",
    "   - Implement constraint satisfaction for complex grammatical validation\n",
    "\n",
    "3. Sanskrit Tokenizer:\n",
    "   - Improve handling of compound words (samāsa)\n",
    "   - Add support for Vedic accents and meter\n",
    "   - Implement subword tokenization optimized for Sanskrit morphology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}